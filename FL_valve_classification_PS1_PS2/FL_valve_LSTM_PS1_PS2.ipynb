{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f126fce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b80e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data shapes: X_ps1=(2205, 6000), X_ps2=(2205, 6000), y=(2205,)\n",
      "Combined data shape: (2205, 12000)\n",
      "Data normalization complete.\n",
      "Reshaped data for LSTM: (2205, 1, 12000)\n",
      "Splitting data into train and test sets...\n",
      "Training data shape: (1984, 1, 12000), Testing data shape: (221, 1, 12000)\n",
      "Class distribution in training set: Counter({0: 1012, 2: 324, 1: 324, 3: 324})\n",
      "Class distribution in testing set: Counter({0: 113, 3: 36, 2: 36, 1: 36})\n",
      "Simulating 3 federated clients...\n",
      "Client 1 - Class distribution: Counter({0: 315, 1: 97, 3: 95, 2: 87}) (Train), Counter({0: 35, 3: 11, 1: 11, 2: 10}) (Test)\n",
      "Creating LSTM model...\n",
      "Model created successfully.\n",
      "Client 1 - Train data shape: (594, 1, 12000), Test data shape: (67, 1, 12000)\n",
      "Client 2 - Class distribution: Counter({0: 297, 2: 101, 3: 99, 1: 97}) (Train), Counter({0: 33, 2: 12, 1: 11, 3: 11}) (Test)\n",
      "Creating LSTM model...\n",
      "Model created successfully.\n",
      "Client 2 - Train data shape: (594, 1, 12000), Test data shape: (67, 1, 12000)\n",
      "Client 3 - Class distribution: Counter({0: 298, 2: 103, 1: 97, 3: 96}) (Train), Counter({0: 34, 1: 11, 3: 11, 2: 11}) (Test)\n",
      "Creating LSTM model...\n",
      "Model created successfully.\n",
      "Client 3 - Train data shape: (594, 1, 12000), Test data shape: (67, 1, 12000)\n",
      "Training and aggregating client models...\n",
      "Training client 1...\n",
      "Training client model...\n",
      "Client model training complete.\n",
      "Evaluating client model on Training data...\n",
      "Client model Training data - Loss: 0.1476, Accuracy: 0.9327\n",
      "Evaluating client model on Testing data...\n",
      "Client model Testing data - Loss: 0.1752, Accuracy: 0.9254\n",
      "Training client 2...\n",
      "Training client model...\n",
      "Client model training complete.\n",
      "Evaluating client model on Training data...\n",
      "Client model Training data - Loss: 0.1411, Accuracy: 0.9293\n",
      "Evaluating client model on Testing data...\n",
      "Client model Testing data - Loss: 0.2025, Accuracy: 0.8806\n",
      "Training client 3...\n",
      "Training client model...\n",
      "Client model training complete.\n",
      "Evaluating client model on Training data...\n",
      "Client model Training data - Loss: 0.1350, Accuracy: 0.9327\n",
      "Evaluating client model on Testing data...\n",
      "Client model Testing data - Loss: 0.0736, Accuracy: 0.9701\n",
      "Client model training and aggregation complete.\n",
      "Client 1 - Final Evaluation on Test Data:\n",
      "Evaluating client model on Final Test data...\n",
      "Client model Final Test data - Loss: 0.1752, Accuracy: 0.9254\n",
      "Client 2 - Final Evaluation on Test Data:\n",
      "Evaluating client model on Final Test data...\n",
      "Client model Final Test data - Loss: 0.2025, Accuracy: 0.8806\n",
      "Client 3 - Final Evaluation on Test Data:\n",
      "Evaluating client model on Final Test data...\n",
      "Client model Final Test data - Loss: 0.0736, Accuracy: 0.9701\n",
      "Creating LSTM model...\n",
      "Model created successfully.\n",
      "Evaluating global model on overall test data...\n",
      "Global model on overall test data - Loss: 1.1535, Accuracy: 0.7964\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    X_ps1 = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\Mahak_btp\\VSCode_FL_Mahak\\FL_Valve_Classification\\PS1.csv\", header=None).values\n",
    "    X_ps2 = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\Mahak_btp\\VSCode_FL_Mahak\\FL_Valve_Classification\\PS2.csv\", header=None).values\n",
    "    y = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\Mahak_btp\\VSCode_FL_Mahak\\FL_Valve_Classification\\valve_target_encoded.csv\", header=None).values.flatten()\n",
    "    print(f\"Data shapes: X_ps1={X_ps1.shape}, X_ps2={X_ps2.shape}, y={y.shape}\")\n",
    "\n",
    "    # Combine features from PS1 and PS2\n",
    "    X_combined = np.concatenate([X_ps1, X_ps2], axis=1)\n",
    "    print(f\"Combined data shape: {X_combined.shape}\")\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_combined)\n",
    "    print(\"Data normalization complete.\")\n",
    "\n",
    "    # Reshape to (samples, timesteps, features) for LSTM input\n",
    "    X_reshaped = X_scaled.reshape(2205, 1, 12000)\n",
    "    print(f\"Reshaped data for LSTM: {X_reshaped.shape}\")\n",
    "\n",
    "    return X_reshaped, y\n",
    "\n",
    "\n",
    "# Define the LSTM model structure\n",
    "def create_model(input_shape):\n",
    "    print(\"Creating LSTM model...\")\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(4, activation=\"softmax\")  # For 4 classes\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    print(\"Model created successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Simulate a federated learning client\n",
    "class FederatedClient:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = create_model(input_shape=(1, 12000))\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        print(\"Training client model...\")\n",
    "        self.model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
    "        print(\"Client model training complete.\")\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def evaluate(self, X_eval, y_eval, set_name=\"Test\"):\n",
    "        print(f\"Evaluating client model on {set_name} data...\")\n",
    "        loss, accuracy = self.model.evaluate(X_eval, y_eval, verbose=0)\n",
    "        print(f\"Client model {set_name} data - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        return loss, accuracy\n",
    "\n",
    "\n",
    "# Main federated learning simulation\n",
    "def federated_learning_simulation(num_clients):\n",
    "    # Load data\n",
    "    X_total, y_total = load_data()\n",
    "\n",
    "    # Split the data for train and test, stratified by class labels\n",
    "    print(\"Splitting data into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_total, y_total, test_size=0.10, random_state=104, stratify=y_total)\n",
    "    print(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n",
    "\n",
    "    # Check class distribution in train and test sets\n",
    "    print(f\"Class distribution in training set: {Counter(y_train)}\")\n",
    "    print(f\"Class distribution in testing set: {Counter(y_test)}\")\n",
    "\n",
    "    # Split data across clients, ensuring fairness\n",
    "    print(f\"Simulating {num_clients} federated clients...\")\n",
    "    data_per_client = len(X_train) // num_clients\n",
    "    clients = []\n",
    "    for i in range(num_clients):\n",
    "        X_client = X_train[i * data_per_client:(i + 1) * data_per_client]\n",
    "        y_client = y_train[i * data_per_client:(i + 1) * data_per_client]\n",
    "\n",
    "        # Split each client data into training and testing sets\n",
    "        X_client_train, X_client_test, y_client_train, y_client_test = train_test_split(\n",
    "            X_client, y_client, test_size=0.1, random_state=42, stratify=y_client)\n",
    "\n",
    "        # Verify client-level class distribution\n",
    "        print(f\"Client {i + 1} - Class distribution: {Counter(y_client_train)} (Train), {Counter(y_client_test)} (Test)\")\n",
    "\n",
    "        client = FederatedClient(X_client, y_client)\n",
    "        clients.append((client, X_client_train, y_client_train, X_client_test, y_client_test))\n",
    "        print(f\"Client {i + 1} - Train data shape: {X_client_train.shape}, Test data shape: {X_client_test.shape}\")\n",
    "\n",
    "    # Train each client model and aggregate weights\n",
    "    global_weights = None\n",
    "    print(\"Training and aggregating client models...\")\n",
    "    for i, (client, X_client_train, y_client_train, X_client_test, y_client_test) in enumerate(clients):\n",
    "        print(f\"Training client {i + 1}...\")\n",
    "        client_weights = client.train(X_client_train, y_client_train)\n",
    "        \n",
    "        # Evaluate client on its own training and testing data\n",
    "        train_loss, train_accuracy = client.evaluate(X_client_train, y_client_train, set_name=\"Training\")\n",
    "        test_loss, test_accuracy = client.evaluate(X_client_test, y_client_test, set_name=\"Testing\")\n",
    "\n",
    "        # Aggregate client weights without training the global model\n",
    "        if global_weights is None:\n",
    "            global_weights = client_weights\n",
    "        else:\n",
    "            # Average weights for federated aggregation\n",
    "            global_weights = [(g + c) / 2 for g, c in zip(global_weights, client_weights)]\n",
    "    \n",
    "    print(\"Client model training and aggregation complete.\")\n",
    "\n",
    "    # Print accuracies for each client\n",
    "    for i, (client, X_client_train, y_client_train, X_client_test, y_client_test) in enumerate(clients):\n",
    "        print(f\"Client {i + 1} - Final Evaluation on Test Data:\")\n",
    "        client.evaluate(X_client_test, y_client_test, set_name=\"Final Test\")\n",
    "\n",
    "    # Set global model with aggregated weights (but no global training)\n",
    "    global_model = create_model(input_shape=(1, 12000))\n",
    "    global_model.set_weights(global_weights)\n",
    "\n",
    "    # Evaluate global model on the original test data (using aggregated weights)\n",
    "    print(\"Evaluating global model on overall test data...\")\n",
    "    test_loss, test_accuracy = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Global model on overall test data - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Number of simulated clients\n",
    "    NUM_CLIENTS = 3\n",
    "    federated_learning_simulation(NUM_CLIENTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d01dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.8643\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9351d94a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13344\\2985682371.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# from tensorflow.python import keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# from tensorflow.python.layers import layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtpu\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\saved_model\\saved_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\saved_model\\builder.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder_impl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_SavedModelBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder_impl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSavedModelBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaved_model_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaver_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0m_np_bfloat16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0m_np_float8_e4m3fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat8_e4m3fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0m_np_float8_e5m2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat8_e5m2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    X_ps1 = pd.read_csv(r\"C:\\Users\\SystemDynamicsLab\\OneDrive - IIT Indore\\Desktop\\Mahak_BTP_Folder\\Mahak\\Mahak_btp\\Dataset_hydraulic-20240917T065446Z-001\\Dataset_hydraulic\\FL_valve_classification_PS1_PS2\\PS1.csv\", header=None).values\n",
    "    X_ps2 = pd.read_csv(r\"C:\\Users\\SystemDynamicsLab\\OneDrive - IIT Indore\\Desktop\\Mahak_BTP_Folder\\Mahak\\Mahak_btp\\Dataset_hydraulic-20240917T065446Z-001\\Dataset_hydraulic\\FL_valve_classification_PS1_PS2\\PS2.csv\", header=None).values\n",
    "    y = pd.read_csv(r\"C:\\Users\\SystemDynamicsLab\\OneDrive - IIT Indore\\Desktop\\Mahak_BTP_Folder\\Mahak\\Mahak_btp\\Dataset_hydraulic-20240917T065446Z-001\\Dataset_hydraulic\\FL_valve_classification_PS1_PS2\\valve_target_encoded.csv\", header=None).values.flatten()\n",
    "    print(f\"Data shapes: X_ps1={X_ps1.shape}, X_ps2={X_ps2.shape}, y={y.shape}\")\n",
    "\n",
    "    # Combine features from PS1 and PS2\n",
    "    X_combined = np.concatenate([X_ps1, X_ps2], axis=1)\n",
    "    print(f\"Combined data shape: {X_combined.shape}\")\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_combined)\n",
    "    print(\"Data normalization complete.\")\n",
    "\n",
    "    # Reshape to (samples, timesteps, features) for LSTM input\n",
    "    X_reshaped = X_scaled.reshape(2205, 1, 12000)\n",
    "    print(f\"Reshaped data for LSTM: {X_reshaped.shape}\")\n",
    "\n",
    "    return X_reshaped, y\n",
    "\n",
    "\n",
    "# Define the LSTM model structure\n",
    "def create_model(input_shape):\n",
    "    print(\"Creating LSTM model...\")\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(4, activation=\"softmax\")  # For 4 classes\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    print(\"Model created successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Simulate a federated learning client\n",
    "class FederatedClient:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = create_model(input_shape=(1, 12000))\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        print(\"Training client model...\")\n",
    "        self.model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
    "        print(\"Client model training complete.\")\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def evaluate(self, X_eval, y_eval, set_name=\"Test\"):\n",
    "        print(f\"Evaluating client model on {set_name} data...\")\n",
    "        loss, accuracy = self.model.evaluate(X_eval, y_eval, verbose=0)\n",
    "        print(f\"Client model {set_name} data - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        return loss, accuracy\n",
    "\n",
    "\n",
    "# Main federated learning simulation\n",
    "def federated_learning_simulation(num_clients):\n",
    "    # Load data\n",
    "    X_total, y_total = load_data()\n",
    "\n",
    "    # Split the data for train and test, stratified by class labels\n",
    "    print(\"Splitting data into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_total, y_total, test_size=0.10, random_state=104, stratify=y_total)\n",
    "    print(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n",
    "\n",
    "    # Check class distribution in train and test sets\n",
    "    print(f\"Class distribution in training set: {Counter(y_train)}\")\n",
    "    print(f\"Class distribution in testing set: {Counter(y_test)}\")\n",
    "\n",
    "    # Split data across clients, ensuring fairness\n",
    "    print(f\"Simulating {num_clients} federated clients...\")\n",
    "    data_per_client = len(X_train) // num_clients\n",
    "    clients = []\n",
    "    for i in range(num_clients):\n",
    "        X_client = X_train[i * data_per_client:(i + 1) * data_per_client]\n",
    "        y_client = y_train[i * data_per_client:(i + 1) * data_per_client]\n",
    "\n",
    "        # Split each client data into training and testing sets\n",
    "        X_client_train, X_client_test, y_client_train, y_client_test = train_test_split(\n",
    "            X_client, y_client, test_size=0.1, random_state=42, stratify=y_client)\n",
    "\n",
    "        # Verify client-level class distribution\n",
    "        print(f\"Client {i + 1} - Class distribution: {Counter(y_client_train)} (Train), {Counter(y_client_test)} (Test)\")\n",
    "\n",
    "        client = FederatedClient(X_client, y_client)\n",
    "        clients.append((client, X_client_train, y_client_train, X_client_test, y_client_test))\n",
    "        print(f\"Client {i + 1} - Train data shape: {X_client_train.shape}, Test data shape: {X_client_test.shape}\")\n",
    "\n",
    "    # Train each client model and aggregate weights\n",
    "    global_weights = None\n",
    "    print(\"Training and aggregating client models...\")\n",
    "    for i, (client, X_client_train, y_client_train, X_client_test, y_client_test) in enumerate(clients):\n",
    "        print(f\"Training client {i + 1}...\")\n",
    "        client_weights = client.train(X_client_train, y_client_train)\n",
    "        \n",
    "        # Evaluate client on its own training and testing data\n",
    "        train_loss, train_accuracy = client.evaluate(X_client_train, y_client_train, set_name=\"Training\")\n",
    "        test_loss, test_accuracy = client.evaluate(X_client_test, y_client_test, set_name=\"Testing\")\n",
    "\n",
    "        # Aggregate client weights without training the global model\n",
    "        if global_weights is None:\n",
    "            global_weights = client_weights\n",
    "        else:\n",
    "            # Average weights for federated aggregation\n",
    "            global_weights = [(g + c) / 2 for g, c in zip(global_weights, client_weights)]\n",
    "    \n",
    "    print(\"Client model training and aggregation complete.\")\n",
    "\n",
    "    # Print accuracies for each client\n",
    "    for i, (client, X_client_train, y_client_train, X_client_test, y_client_test) in enumerate(clients):\n",
    "        print(f\"Client {i + 1} - Final Evaluation on Test Data:\")\n",
    "        client.evaluate(X_client_test, y_client_test, set_name=\"Final Test\")\n",
    "\n",
    "    # Set global model with aggregated weights (but no global training)\n",
    "    global_model = create_model(input_shape=(1, 12000))\n",
    "    global_model.set_weights(global_weights)\n",
    "\n",
    "    # Evaluate global model on the original test data (using aggregated weights)\n",
    "    print(\"Evaluating global model on overall test data...\")\n",
    "    test_loss, test_accuracy = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Global model on overall test data - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Number of simulated clients\n",
    "    NUM_CLIENTS = 3\n",
    "    federated_learning_simulation(NUM_CLIENTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "763383da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.14.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb3c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (2.14.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp39-cp39-win_amd64.whl (7.5 kB)\n",
      "Collecting tensorflow-intel==2.18.0\n",
      "  Downloading tensorflow_intel-2.18.0-cp39-cp39-win_amd64.whl (390.0 MB)\n",
      "     -------------------------------------- 390.0/390.0 MB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.12.1-cp39-cp39-win_amd64.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Collecting numpy<2.1.0,>=1.26.0\n",
      "  Downloading numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "     ---------------------------------------- 15.9/15.9 MB 4.3 MB/s eta 0:00:00\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0\n",
      "  Downloading ml_dtypes-0.4.1-cp39-cp39-win_amd64.whl (126 kB)\n",
      "     -------------------------------------- 126.7/126.7 kB 7.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.24.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.59.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (68.2.2)\n",
      "Collecting keras>=3.5.0\n",
      "  Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Collecting tensorboard<2.19,>=2.18\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: namex in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Collecting rich\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "     -------------------------------------- 242.4/242.4 kB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: optree in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.18.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: numpy, mdurl, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.6\n",
      "    Uninstalling numpy-1.21.6:\n",
      "      Successfully uninstalled numpy-1.21.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\SystemDynamicsLab\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\SystemDynamicsLab\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\~~mpy\\\\.libs\\\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eca92eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\systemdynamicslab\\appdata\\roaming\\python\\python39\\site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc9580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
